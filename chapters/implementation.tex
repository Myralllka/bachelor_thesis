\chapter{Implementation}
\label{chapter:implementation}

In this section there is a description of the implemented solution: hardware in the \autoref{sec:impl_hardware}, used software tools in the \autoref{sec:impl_software} and integration of all parts together in the \autoref{sec:impl_integration}. The code can be found on GitHub: stereo pair driver\footnote{\url{https://github.com/Myralllka/UAV_basler_stereopair_driver}} and the main module\footnote{\url{https://github.com/Myralllka/UAV_localisation_from_cameras}}.

\section{Hardware}
\label{sec:impl_hardware}
\begin{figure}[h]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/CAD.jpg}
    \caption{The CAD model of the prototype}
    \label{fig:proto_scheme}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/prototype.jpg}
    \caption{The printed prototype}
    \label{fig:proto_printed}
  \end{subfigure}
  \label{The proposed sollution prototype}
\end{figure}

The camera holder CAD model was created in a Fusion360 software considering requirements of rotation $90^\circ$ and the distance close to the average MAV sizes, the screenshot from modelling process is in \autoref{fig:proto_scheme}.
The 3d-printed solution prototype with cameras mounted is in the \autoref{fig:proto_printed}

Basler daA1600-60um cameras were chosen for this project because they have a global shutter to capture moving objects, good image quality and they are not so expensive.
\begin{center}
    \begin{tabular}{ l l }
    \hline
    name                   & property          \\ \hline
    Lens mount type        & S-mount           \\
    Data transfer protocol & USB 3.0           \\
    Max. frame rate        & 60 fps            \\
    Resolution (HxV)       & 1600 px x 1200 px \\
    Resolution             & 2 MP              \\
    Price                  & 289.00 EUR        \\ \hline
    \end{tabular}
\end{center}

Cameras' lences have $120^\circ$ FOV, which gives enough overlapping zone to detect features in $\sigma$ (see \autoref{fig:sch_stereo}).

Intel NUC with 8 cores CPU is used as the on-board computer for MAV. 
No external GPU is needed, so all algorithms should take it into consideration.

\section{Software tools}
\label{sec:impl_software}

The proposed solution uses Robotic operating system (ROS)\cite{Rospaper} as the middleware.
ROS is the whole ecosystem with hundrets of already implemented algorithms and libraries to interact with robots between Its parts and sensors.
It is an opens-ource tool, so the apriltag detector and camera driver used in this thesis are based on official modules from ROS community.

The MRS UAV system \cite{Baca2021} is used as a drone control environment. It is based on ROS, but it is the special framework for MAV's to implement and test path planning, control, computer vision, objects tracking and much more problems related to MAV's.

OpenCV \cite{opencv} is an open-source library for computer vision.
Some of algorithms used in this thesis are taken from this library, PnP (\autoref{sec:pnp}) and improved version of triangulation from \autoref{sec:svdtriang} called "The Golden Standard Triangulation Method" which is a combination of SVD triangulation with applied Sampson correction.
Also feature detector and matcher (\autoref{sec:features}) both are used from the OpenCV.

Eigen \cite{Eigen} library is used for all linear algebra. 

\section{Integration}
\label{sec:impl_integration}

Implementation steps are described in the same order as in a \autoref{chapter:methodology}: calibrations, features matching, extracting 3D poses of features, error computation.

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{graphics/calibration.png}
    \caption{The calibration process}
    \label{fig:calib}
\end{figure}

There is a package\footnote{\url{http://wiki.ros.org/camera_calibration}} in ROS ecosystem, that was used for the camera calibration process described in \autoref{sec:meth_calib}. 
It takes a chessboard parameters (square size and number of squares) as input, then in the interactive mode (see \autoref{fig:calib}) it collects images for calibration and as a result creates a file with camera calibration matrix and distortion coeficients, if the lens has it.

There are two implemented methods of a stereo pair calibration described in \autoref{sec:stereocalib}.
Both of them uses an apriltag calibration pattern, shown in the \autoref{fig:aptags}.
The main reason why this pattern was chosen is because each tag can be detected indevidually, so there is no need to have all of them simultaneously in the overlapping zone.
The first method is implemented using the "Least-square estimation of transformation between two point sets" \cite{Umeyama1991} implementation from Eigen.
As the initial poses, measurements from the CAD model are used.
The second method is implemented using the OpenCV PnP solver.
As far as a standard apriltag detector outputs only the 3D poses of detected tags, it was modified to publish an external information needed for the PnP algorithm - 2D coordinates of apriltags' corners in each image.

After both cameras and stereo pair are calibrated, next step is an image synchronisation and features detection, matching and filtering (\autoref{sec:features}). 

Brute-Force matcher is one of the most simple matchers.
It takes one descriptor from one set, somehow computing the distances to all descriptors from the second set, matching them and repeating it to minimize the distance between two sets.
Even after this process, there will be some outliers.
Distance to corresponding epipolar lines can be used to filter them. \autoref{eq:epiconstr} is valid only in an ideal case; otherwise, the equation result will be not $0$ but the distance from the reprojected point to its corresponding epipolar line.


% Usage of proposed aproach: at a timestamp $t_1$ it is already possible to use SfM to make a point cloud from green + red points and blue + red.
% Here the proposed algorithm can help - it gives common points to an algorithm, so instead of SfM from 2 images it does SfM from 4 images but with bigger pressision, because $T_{static}$ remains the same, and the only necessary transformation that should be estimated is $T_{t_0t_1}$, and the innertial module can help with that.

% Another possible approach is to use already working SfM algorithm implementation for a left and rigth camera separately to obtaine two pointclouds, and then align them and fix a scale using Iterative closest point algorithm with respect to fixed red pointcloud
