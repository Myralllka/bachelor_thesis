\chapter{Implementation}
\label{chapter:implementation}

This chapter describes the implementation of the solution.
The used hardware is presented in \autoref{sec:impl_hardware}, the software tools and integration of all parts together in \autoref{sec:impl_software}. The code can be found on GitHub: stereo pair driver\footnote{\url{https://github.com/Myralllka/UAV_basler_stereopair_driver}} and the main module\footnote{\url{https://github.com/Myralllka/UAV_localisation_from_cameras}}.

\section{Hardware}
\label{sec:impl_hardware}
\begin{figure}[ht]
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/CAD.jpg}
    \caption{The CAD model of the prototype.}
    \label{fig:proto_scheme}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/prototype.jpg}
    \caption{The printed prototype}
    \label{fig:proto_printed}
  \end{subfigure}
  \caption{A prototype of the proposed solution.}
  \label{fig:prototype}
\end{figure}

A CAD model of a camera mount was created in the Fusion360 software considering the requirements of a \ang{90} rotation between cameras and distance of the cameras close to the average MAV size.
A picture of the design from the CAD software is in \autoref{fig:proto_scheme}.
The 3D-printed prototype with the cameras mounted is in \autoref{fig:proto_printed}

Basler daA1600-60um cameras were chosen for this project because they have a global shutter which is important when deployed onboard a fast-moving platform, good image quality, and high framerate, making them suitable for the proposed method based on the assumptions defined in \autoref{sec:problem_definition}. 
More details regarding camera parameters are in \autoref{tab:daA1600}.

\begin{table}
    \begin{center}
      \begin{tabular}{ l l }
      \hline
      Parameter              & Value             \\ \hline
      Lens mount type        & S-mount           \\
      Data transfer protocol & USB 3.0           \\
      Max. frame rate        & \SI{60}{fps}            \\
      Resolution (HxV)       & 1600 px x 1200 px \\
      Resolution             & 2 MP              \\
      Price                  & 289.00 EUR        \\ \hline
      \end{tabular}
    \end{center}
    \caption{daA1600-60um specifications.}
    \label{tab:daA1600}
\end{table}

Lenses of the cameras were chosen so that the resulting horizontal FOV is approximately \ang{120}, which provides a sufficient overlapping zone to detect features in $\Psi$ (see \autoref{fig:sch_stereo}).

The solution was developed and tested on the Lenovo ThinkPad X280 with Intel(R) Core i7-8550U CPU and 16Gb RAM.
Intel NUC with Intel(R) Core i7-10710U CPU and 16Gb RAM is used as the onboard computer for the MAV.
No dedicated GPU is needed.

\section{Software tools}
\label{sec:impl_software}

The proposed solution uses the Robot operating system (ROS) \cite{Rospaper} as a middleware.
ROS is an open-source ecosystem with hundreds of already implemented algorithms and libraries to interact between different parts of a robot's system, such as sensor drivers, image processing algorithms, planners, controllers etc.
The AprilTag detector and a driver for Basler cameras used in this thesis are based on publicly available modules from the ROS community.

The MRS UAV system \cite{Baca2021} is used as a drone control environment. It is based on ROS, but it is a unique framework for MAVs to implement and test path planning, control, computer vision, object tracking, and many more MAV-related problems.

The OpenCV\footnote{\url{https://opencv.org/}} open-source computer vision library and the Eigen\footnote{\url{http://eigen.tuxfamily.org}} open-source library for efficient linear algebra were used for the implementation.
PCL\footnote{\url{https://pointclouds.org/}} which stands for Point Cloud Library is an open-source library used for point cloud processing.
All of the software presented in this thesis was implemented in C++.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{graphics/calibration.png}
    \caption{The single camera calibration process.}
    \label{fig:calib}
\end{figure}

Calibration of parameters of the camera projection and distortion models (described in \autoref{sec:meth_calib}) was performed using the ROS Camera Calibration package\footnote{\url{http://wiki.ros.org/camera_calibration}}.
A chessboard pattern is used for the calibration, and its physical parameters must be specified to the calibration program.
The calibration process itself is done interactively using a graphical user interface (shown in \autoref{fig:calib}).
The interface displays the progress of the process to the user, and when a sufficient dataset is obtained, the user can trigger the parameter optimization. 
Results of the optimization are then saved to a file for later use.

The two calibration methods described in \autoref{sec:stereocalib} were both implemented to use an AprilTag calibration pattern, shown in \autoref{fig:aptags}.
The main reason why this pattern was chosen is that each tag can be detected individually, so there is no need to have all of them simultaneously in the overlapping zone.
The first method, described in \autoref{sec:lsq_umeyama}, is implemented using the "Least-square estimation of transformation between two point sets" \cite{Umeyama1991} implementation from Eigen.
Measurements from the CAD model of the camera mount were used as the initial estimate of the cameras' relative poses.
The second method, described in \autoref{sec:pnp}, is implemented using the OpenCV PnP solver.
Because the publicly available implementation of the AprilTag detector outputs only 3D poses of detected tags, the detector was modified to output also 2D coordinates of corners of the detected markers, which is needed for the PnP algorithm and during the debug session for computing the reprojection error.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{graphics/corresp.png}
  \caption[The result of features detection, matching, and outliers filtering.]{The result of feature detection, matching, and outliers filtering. There are cropped segments of images from the left camera (left half) and right camera (right half). Detected and filtered features are marked as colored circles, and matched correspondence pairs are connected with lines of the same color as their points.}
  \label{fig:corresp_life}
\end{figure}

After both cameras and the stereo pair are calibrated, the following steps are feature detection, matching, and filtering, described in \autoref{sec:features}.
The example from a real-world experiment is in \autoref{fig:corresp_life}.

The synchronization is done on a stereo pair driver level.
As described in \autoref{sec:features}, the ORB features detector and a brute-force matcher implementations from the OpenCV were used for feature detection and matching.
The next step is reconstructing the 3D scene from the obtained feature pairs.
The method described in \autoref{sec:shortest_distance} is implemented from scratch, and the implementation of triangulation described in \autoref{sec:svdtriang} is taken from OpenCV.